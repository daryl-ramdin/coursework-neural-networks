{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d298afb",
   "metadata": {},
   "source": [
    "Impoort the necessary libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e67c6eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/darylramdin/opt/anaconda3/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/darylramdin/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/darylramdin/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/darylramdin/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/darylramdin/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b88f2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import truncnorm\n",
    "from sklearn import datasets as ds\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179ded5",
   "metadata": {},
   "source": [
    "Create the Optimizer classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3243f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def optimize(self,w,learning_rate,dw):\n",
    "        return\n",
    "\n",
    "    '''\n",
    "    Any initialization needed for dropouts\n",
    "    '''\n",
    "    def initialise_dropout(self,b_node_indices, node_indices):\n",
    "        return\n",
    "\n",
    "    '''\n",
    "    Some optimizers need to be aware of any dropouts that may be taking place\n",
    "    '''\n",
    "    def dropout(self,back_active_node_indices, active_node_indices,dw):\n",
    "        return\n",
    "\n",
    "class GradientDescentOptimizer(Optimizer):\n",
    "    def optimize(self,w,learning_rate,dw):\n",
    "        #Update the weights\n",
    "        w -= (learning_rate * dw)\n",
    "        return w\n",
    "\n",
    "class AdamOptimizer(Optimizer):\n",
    "    def __init__(self, beta1=0.9,beta2=0.99):\n",
    "        super().__init__()\n",
    "        self.master_moment1 = np.array([0]).astype(float)\n",
    "        self.master_moment2 = np.array([0]).astype(float)\n",
    "        self.moment1 = np.array([0]).astype(float)\n",
    "        self.moment2 = np.array([0]).astype(float)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.back_node_count = 0\n",
    "        self.node_count = 0\n",
    "        self.active_node_indices = np.array([0])\n",
    "        self.moments_created = False\n",
    "\n",
    "    def initialise_dropout(self,back_node_count, node_count):\n",
    "        if self.moments_created==True: return\n",
    "        self.moments_created = True\n",
    "        self.back_node_count = back_node_count\n",
    "        self.node_count = node_count\n",
    "        self.active_node_indices = np.array(range(0, self.node_count))\n",
    "        self.back_active_node_indices = np.array(range(0, self.back_node_count))\n",
    "        self.master_moment1 = np.zeros([self.node_count,self.back_node_count]).astype(float)\n",
    "        self.master_moment2 = np.zeros([self.node_count,self.back_node_count]).astype(float)\n",
    "\n",
    "    def dropout(self,back_active_node_indices, active_node_indices,dw):\n",
    "        self.back_active_node_indices = back_active_node_indices\n",
    "        self.active_node_indices = active_node_indices\n",
    "        self.moment1 = self.master_moment1[:, back_active_node_indices]\n",
    "        self.moment1 = self.moment1[active_node_indices, :]\n",
    "        self.moment2 = self.master_moment2[:, back_active_node_indices]\n",
    "        self.moment2 = self.moment2[active_node_indices, :]\n",
    "\n",
    "\n",
    "    def optimize(self,w,learning_rate,dw):\n",
    "        #Calculate the value of the active moments. As we are implementing\n",
    "        #dropout regularisation, we need to also drop out the corresponding\n",
    "        #moments associated with each weight\n",
    "\n",
    "        self.moment1 = self.beta1 * self.moment1 + (1 - self.beta1) * dw\n",
    "        self.moment2 = self.beta2 * self.moment2 + (1 - self.beta2) * np.square(dw)\n",
    "\n",
    "\n",
    "\n",
    "        #Update the master moments\n",
    "        temp = self.master_moment1.copy()[:,self.back_active_node_indices]\n",
    "        temp[self.active_node_indices] = self.moment1\n",
    "        self.master_moment1[:, self.back_active_node_indices] = temp\n",
    "\n",
    "        temp = self.master_moment2.copy()[:,self.back_active_node_indices]\n",
    "        temp[self.active_node_indices] = self.moment2\n",
    "        self.master_moment2[:, self.back_active_node_indices] = temp\n",
    "\n",
    "        #Calculate the new weights\n",
    "        w -= learning_rate * self.moment1 / (np.sqrt(self.moment2) + 1e-7)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55226b6e",
   "metadata": {},
   "source": [
    "Activation Function Classes. These allow for the implementation of Relu and Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "286d505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    def __init__(self,type):\n",
    "        self.type = type\n",
    "\n",
    "    def apply(self,x):\n",
    "        return np.array([0])\n",
    "\n",
    "    def derivative(self,x):\n",
    "        return\n",
    "\n",
    "class SigmoidActivation(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"sigmoid\")\n",
    "\n",
    "    def apply(self,x):\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "    def derivative(self,x):\n",
    "        return self.apply(x) * (1.0 - self.apply(x))\n",
    "\n",
    "class ReluActivation(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"relu\")\n",
    "\n",
    "    def apply(self,x):\n",
    "        return max(0,x)\n",
    "\n",
    "    def derivative(self,x):\n",
    "        return (x>=0) * 1 #From INM702 lab06_2 solution\n",
    "\n",
    "class LinearActivation(ActivationFunction):\n",
    "    def __init__(self,m,c):\n",
    "        super().__init__(\"linear\")\n",
    "        self.m = m\n",
    "        self.c = c\n",
    "\n",
    "    def apply(self,x):\n",
    "        return self.c+(self.m*x)\n",
    "\n",
    "    def derivative(self,x):\n",
    "        return (x*self.m)/self.m\n",
    "\n",
    "class SoftmaxActivation(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"softmax\")\n",
    "\n",
    "    def apply(self,x):\n",
    "        x_shift = x - np.max(x)\n",
    "        ex = np.exp(x_shift)\n",
    "        return ex / ex.sum(axis=0, keepdims=True)\n",
    "\n",
    "    def derivative(self,x):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad88a1",
   "metadata": {},
   "source": [
    "The Layer classes. These represent Layers in the Neural Network. While not usually considered a layer in a Neural Network, an InputLayer class is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d131e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    '''\n",
    "    This is the base class for all layers in the neural network, including the input layer.\n",
    "    While the input layer is not necessarily considered part of the layer count, we use this class\n",
    "    to store the inputs as it is connected to the first hidden layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, layer_id: str, number_of_nodes: int):\n",
    "        #All layers are active\n",
    "        self.layer_id = layer_id\n",
    "        self.number_of_nodes = number_of_nodes\n",
    "        self.number_of_active_nodes = self.number_of_nodes\n",
    "        self.output_original = np.empty([0]).astype(float)\n",
    "        self.output = self.output_original\n",
    "        self.biased_output = np.empty([0]).astype(float)\n",
    "        self.active_node_indices = np.array(range(0,self.number_of_active_nodes))\n",
    "\n",
    "    def describe(self):\n",
    "        print(\"Layer ID:\", self.layer_id, \"\\nNumber of Nodes:\", self.number_of_nodes, \",Number of Active Nodes:\", self.number_of_active_nodes)\n",
    "\n",
    "    def addbias(self):\n",
    "        #Add the bias\n",
    "        bias = np.ones(self.output.shape[0])\n",
    "        bias = bias.reshape(-1, 1)\n",
    "        self.biased_output = np.append(self.output, bias, axis=1)\n",
    "\n",
    "    def dropout(self,active_percent: float):\n",
    "        self.number_of_active_nodes = int((1-(active_percent/100)) * self.number_of_nodes)\n",
    "        self.active_node_indices = sorted(random.sample(range(0, self.number_of_nodes),self.number_of_active_nodes))\n",
    "        if self.layer_id == \"In\": self.output = self.output_original[:,self.active_node_indices]\n",
    "        return\n",
    "\n",
    "    def reset_active_nodes(self):\n",
    "        self.number_of_active_nodes = self.number_of_nodes\n",
    "        self.active_node_indices = np.array(range(0, self.number_of_active_nodes))\n",
    "        if self.layer_id == \"In\": self.output = self.output_original[:, self.active_node_indices]\n",
    "\n",
    "class InputLayer(Layer):\n",
    "    #This is used to store the sample data\n",
    "    def __init__(self,layer_id: str, number_of_nodes: int):\n",
    "        '''\n",
    "        :param layer_id: An identified for this layer\n",
    "        :param data: The sample data. Must be of the shape [number of samples,number of features].\n",
    "                     The number of nodes in the input layer is given by the number of features\n",
    "        '''\n",
    "        super().__init__(layer_id, number_of_nodes) # for the bias\n",
    "\n",
    "    def set_data(self,data: np.array):\n",
    "        '''\n",
    "        :param data: Array of shape [number of samples,number of features]\n",
    "        :return:\n",
    "        '''\n",
    "        self.output_original = data\n",
    "        self.output = self.output_original\n",
    "        #Set the bias\n",
    "        self.addbias()\n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "    #This layer is used as the base for the Hidden and Output layers as they use\n",
    "    #weights and activation functions\n",
    "    def __init__(self, layer_id: str, number_of_nodes: int, activation_function: ActivationFunction, opti: Optimizer, learning_rate: float):\n",
    "        super().__init__(layer_id, number_of_nodes) #for the bias\n",
    "        #Initialise the partial derivatives\n",
    "        self.d_l_a = np.array([0]).astype(float) #Derivative of loss wrt to activation\n",
    "        self.d_a_z = np.array([0]).astype(float) #Derivative of activation\n",
    "        self.d_z_w = np.array([0]).astype(float) #Derivative of z wrt to weights\n",
    "        self.d_l_z = np.array([0]).astype(float) #Derivatie of loss wrt to z\n",
    "        self.d_l_w = np.array([0]).astype(float) #Derivative of loss wrt to weights\n",
    "\n",
    "        #Initialise the back layer\n",
    "        self.back_layer = None\n",
    "        self.front_layer = None\n",
    "\n",
    "        #Set the activation function and learning rate\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        #Initialise the weights and store the learning rate\n",
    "        self.master_weights = np.empty([0]).astype(float)\n",
    "        self.active_weights = self.master_weights\n",
    "        self.weighted_input = np.array([0]).astype(float)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        #Initialise the arrays for logging the history of changes\n",
    "        self.weight_history = np.empty([0]).astype(float)\n",
    "        self.activation_history = np.empty([0]).astype(float)\n",
    "\n",
    "        self.optimizer= opti\n",
    "\n",
    "    def connect(self, back: Layer, front: Layer = None):\n",
    "        #set the back layer\n",
    "        self.back_layer = back\n",
    "        self.front_layer = front\n",
    "        return\n",
    "\n",
    "    def create_weight_matrices(self):\n",
    "        #Initialize the weight matrices of the neural network\n",
    "        #This code was based on INM 702 (2022), Lab06_3, City, University of London\n",
    "        #https://moodle.city.ac.uk/pluginfile.php/2974721/mod_folder/content/0/Lab06_3.ipynb?forcedownload=1\n",
    "        np.random.seed(42)\n",
    "        rad = 1 / np.sqrt(self.back_layer.number_of_nodes)\n",
    "        mean=0\n",
    "        sd=1\n",
    "        low=-rad\n",
    "        upp=rad\n",
    "        w = truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "        self.master_weights = w.rvs((self.number_of_active_nodes, self.back_layer.number_of_active_nodes),random_state=42)\n",
    "        self.active_weights = self.master_weights\n",
    "\n",
    "    def update_weight_matrix(self):\n",
    "        # Let's get the weight indices to dropout\n",
    "        self.active_weights = self.master_weights[:, self.back_layer.active_node_indices]\n",
    "        self.active_weights = self.active_weights[self.active_node_indices, :]\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        #Apply the weights to the inputs\n",
    "        self.weighted_input = np.dot(self.active_weights,self.back_layer.output.T)  # number_of_nodes x number_of_samples\n",
    "\n",
    "        #Apply the activation function\n",
    "        self.output = self.activation_function.apply(self.weighted_input)  # number_of_nodes x number of samples\n",
    "\n",
    "        #Transpose so that our array shape is standard\n",
    "        self.output = np.transpose(self.output)  # number of samples x number_of_nodes\n",
    "\n",
    "        return\n",
    "\n",
    "    def backward_propagate(self):\n",
    "        return\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.optimizer.initialise_dropout(self.back_layer.number_of_nodes,self.number_of_nodes)\n",
    "        self.optimizer.dropout(self.back_layer.active_node_indices,self.active_node_indices,self.d_l_w)\n",
    "        self.active_weights = self.optimizer.optimize(self.active_weights,self.learning_rate,self.d_l_w)\n",
    "        #Update the weights\n",
    "        #self.active_weights = self.active_weights - (self.learning_rate * self.d_l_w)  # number_of_active_nodes x number_active_nodes_back_layer\n",
    "        # The next step is to update the master weights with their corresponding changes\n",
    "        # in the active weights\n",
    "        temp = self.master_weights.copy()[:,self.back_layer.active_node_indices]\n",
    "        temp[self.active_node_indices] = self.active_weights\n",
    "        self.master_weights[:, self.back_layer.active_node_indices] = temp\n",
    "        return\n",
    "\n",
    "    def reset_active_nodes(self):\n",
    "        super().reset_active_nodes()\n",
    "        self.active_weights = self.master_weights\n",
    "\n",
    "    def delta_l_a(self):\n",
    "        return self.d_l_a # number_of_samples x number of nodes\n",
    "\n",
    "    def delta_a_z(self):\n",
    "        return self.d_a_z # number_of_samples x number of nodes\n",
    "\n",
    "    def delta_z_w(self):\n",
    "        return self.d_z_w # number_of_samples x number_nodes_back_layer\n",
    "\n",
    "    def delta_l_z(self):\n",
    "        return self.d_l_z # number_of_samples x number of nodes\n",
    "\n",
    "    def delta_z_a(self):\n",
    "        return self.active_weights #number of nodes x number_nodes_back_layer\n",
    "\n",
    "class HiddenLayer(ActivationLayer):\n",
    "    def __init__(self, layer_id: str, number_of_nodes: int, activation_function: ActivationFunction,opti: Optimizer, learning_rate: float):\n",
    "        super().__init__(layer_id,number_of_nodes,activation_function,opti,learning_rate)\n",
    "        self.front_layer = None\n",
    "\n",
    "    def describe(self):\n",
    "        super().describe()\n",
    "        print(\"Back Layer:\",self.back_layer.layer_id, \"Front Layer:\", self.front_layer.layer_id)\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        super().forward_propagate()\n",
    "\n",
    "        #Uncomment the line below to use the bias\n",
    "        #self.addbias()\n",
    "\n",
    "    def backward_propagate(self):\n",
    "        self.front_d_l_z = self.front_layer.delta_l_z()  # number of samples x number_nodes_front_layer\n",
    "        self.front_d_z_a = self.front_layer.delta_z_a()  # number_nodes_front_layer x number_nodes\n",
    "        # Get d_l_a\n",
    "        self.d_l_a = np.dot(self.front_d_l_z, self.front_d_z_a)  # number_samples x number_of_nodes\n",
    "        # Get d_a_z\n",
    "        self.d_a_z = self.activation_function.derivative(self.weighted_input.T)  # number_of_samples x number_of_nodes\n",
    "        # Get d_l_z\n",
    "        self.d_l_z = self.d_l_a * self.d_a_z  # number_of_samples x number_of_nodes\n",
    "        # Finally d_l_w, Note self.back_layer.forward_output = d_z_w\n",
    "        self.d_l_w = np.dot(self.d_l_z.T, self.back_layer.output) #/ self.back_layer.output.shape[0]  # number_of_nodes x number_nodes_back_layer\n",
    "\n",
    "class OutputLayer(ActivationLayer):\n",
    "    def __init__(self, layer_id: str, number_of_nodes: int, activation_function: ActivationFunction,opti: Optimizer, learning_rate: float):\n",
    "        super().__init__(layer_id,number_of_nodes,activation_function,opti,learning_rate)\n",
    "        self.target = np.array([0]).astype(float)\n",
    "\n",
    "    def describe(self):\n",
    "        super().describe()\n",
    "        print(\"Back Layer:\",self.back_layer.layer_id)\n",
    "\n",
    "    def set_target(self,target: np.array):\n",
    "        self.target = target\n",
    "\n",
    "    def back_propagate(self):\n",
    "        if self.activation_function.type == \"softmax\":\n",
    "            self.d_l_z = self.output - self.target  # number_of_samples x number_of_nodes\n",
    "            # Get d_l_w = d_l_z . d_z_w. Note d_z_w = self.back_layer.forward_output\n",
    "            self.d_l_w = np.dot(self.d_l_z.T, self.back_layer.output) #/ self.back_layer.output.shape[0]  # number_of_nodes x number_nodes_back_layer\n",
    "        else:\n",
    "            self.d_l_a = self.target - self.output  # number_of_samples x number_of_nodes\n",
    "            self.d_a_z = self.activation_function.derivative(self.weighted_input.T)  # number_of_samples x number_of_nodes\n",
    "            self.d_l_z = self.d_l_a * self.d_a_z  # number_of_samples x number_of_nodes\n",
    "            self.d_z_w = self.back_layer.output  # number_of_samples x number_nodes_back_layer\n",
    "            self.d_l_w = np.dot(self.d_l_z.T, self.d_z_w)  # number_of_nodes x number_nodes_back_layer\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75b42e",
   "metadata": {},
   "source": [
    "Now the Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8120b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork:\n",
    "\n",
    "    '''\n",
    "    Initialise all member variables in the constructor\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        #Stores the input layer\n",
    "        self.input_layer = None\n",
    "\n",
    "        #A numpy array for storing all the hidden layers\n",
    "        self.hidden_layers = np.empty([0]).astype(HiddenLayer)\n",
    "\n",
    "        #The output layer\n",
    "        self.output_layer = None\n",
    "\n",
    "        #An array for tracking the loss with each forward pass of the network\n",
    "        self.loss_log = []\n",
    "\n",
    "        #An array for tracking the accuracy with each forward pass of the network\n",
    "        self.accuracy_log = []\n",
    "\n",
    "    def describe(self):\n",
    "        self.input_layer.describe()\n",
    "        for l in self.hidden_layers:\n",
    "            l.describe()\n",
    "        self.output_layer.describe()\n",
    "\n",
    "    def add_input(self,input_layer: InputLayer):\n",
    "        '''\n",
    "        Adds the input layer.\n",
    "        :param input_layer: Input Layer object\n",
    "        :return:\n",
    "        '''\n",
    "        self.input_layer = input_layer\n",
    "\n",
    "    def add_output(self,output_layer: OutputLayer):\n",
    "        self.output_layer = output_layer\n",
    "\n",
    "    def add_hidden(self,hidden_layer: HiddenLayer):\n",
    "        self.hidden_layers = np.append(self.hidden_layers,np.array([hidden_layer]), axis=0)\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        for l in self.hidden_layers:\n",
    "            l.forward_propagate()\n",
    "        self.output_layer.forward_propagate()\n",
    "        cost = -np.mean(self.output_layer.target * np.log(self.output_layer.output + 1e-8))\n",
    "        return cost\n",
    "\n",
    "    def backward_propagate(self):\n",
    "        self.output_layer.back_propagate()\n",
    "        for i in range(len(self.hidden_layers)-1,-1,-1):\n",
    "            self.hidden_layers[i].backward_propagate()\n",
    "\n",
    "        #After backpropagation we then update the active_weights\n",
    "        self.output_layer.update_weights()\n",
    "        for i in range(len(self.hidden_layers)-1,-1,-1):\n",
    "            self.hidden_layers[i].update_weights()\n",
    "\n",
    "    def dropout(self,dropout_percentage):\n",
    "        self.input_layer.dropout(dropout_percentage)\n",
    "\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            self.hidden_layers[i].dropout(dropout_percentage)\n",
    "            self.hidden_layers[i].update_weight_matrix()\n",
    "        self.output_layer.update_weight_matrix()\n",
    "\n",
    "    def reset_active_nodes(self):\n",
    "        self.input_layer.reset_active_nodes()\n",
    "\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            self.hidden_layers[i].reset_active_nodes()\n",
    "\n",
    "        self.output_layer.reset_active_nodes()\n",
    "\n",
    "    def build(self):\n",
    "        #Let's connect the layers together\n",
    "        last_index = len(self.hidden_layers)-1\n",
    "        #If this is only a 2 layer network i.e. 1 Hidden and 1 Output then connect them\n",
    "        if last_index == 0:\n",
    "            self.hidden_layers[last_index].connect(self.input_layer,self.output_layer)\n",
    "            self.hidden_layers[last_index].create_weight_matrices()\n",
    "        else:\n",
    "            #There are multiple hidden so we iterate\n",
    "            for i in range(len(self.hidden_layers)):\n",
    "                # This is the first hidden layer so it's back layer is the input layer\n",
    "                if i == 0 :\n",
    "                    self.hidden_layers[i].connect(self.input_layer,self.hidden_layers[i+1])\n",
    "                # This is the last hidden layer so it's front layer is the output layer\n",
    "                elif i == last_index:\n",
    "                    self.hidden_layers[i].connect(self.hidden_layers[i-1], self.output_layer)\n",
    "                else:\n",
    "                    self.hidden_layers[i].connect(self.hidden_layers[i - 1], self.hidden_layers[i+1])\n",
    "                #Create the weight matrices\n",
    "                self.hidden_layers[i].create_weight_matrices()\n",
    "\n",
    "        #The last step is to connect the output layer to the last hidden\n",
    "        self.output_layer.connect(self.hidden_layers[last_index])\n",
    "\n",
    "        # Create the weight matrices\n",
    "        self.output_layer.create_weight_matrices()\n",
    "\n",
    "        return\n",
    "\n",
    "    def train(self, X, y, dropout_rates, epochs):\n",
    "        # This is where were train the model.\n",
    "        # Set the data and target\n",
    "        self.input_layer.set_data(X)\n",
    "        self.output_layer.set_target(y)\n",
    "        accuracy = 0\n",
    "\n",
    "        #print(\"who:\",self.output_layer.master_weights)\n",
    "        for i in range(epochs):\n",
    "            #At the start of each epoch we drop nodes\n",
    "            self.dropout(dropout_rates[i])\n",
    "            cost = self.forward_propagate()\n",
    "            self.backward_propagate()\n",
    "            self.reset_active_nodes()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                self.loss_log.append(cost)\n",
    "                accuracy = self.get_accuracy(self.output_layer.output, y)\n",
    "                self.accuracy_log.append(accuracy)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(\"Accuracy for epoch\", epochs, \":\", accuracy*100)\n",
    "\n",
    "        return self.loss_log, self.accuracy_log\n",
    "\n",
    "    def predict(self, X,y):\n",
    "        # This is where were train the model.\n",
    "        # Set the data and target\n",
    "        self.input_layer.set_data(X)\n",
    "        for l in self.hidden_layers:\n",
    "            l.forward_propagate()\n",
    "        self.output_layer.forward_propagate()\n",
    "        y_pred = self.output_layer.output\n",
    "\n",
    "        return self.get_accuracy(y_pred,y)\n",
    "\n",
    "    '''\n",
    "    Code for the get_accuracy came from \n",
    "    http://www.adeveloperdiary.com/data-science/deep-learning/neural-network-with-softmax-in-python/\n",
    "    '''\n",
    "    def get_accuracy(self,y_pred, y_target):\n",
    "        y_hat = np.argmax(y_pred, axis=1)\n",
    "        y_true = np.argmax(y_target, axis=1)\n",
    "        accuracy = (y_hat == y_true).mean()\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89240b",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15bdb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_train_test():\n",
    "    digits = ds.load_digits()\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    y = np.reshape(y, (-1, 1))\n",
    "    y = onehot_encoder.fit_transform(y)\n",
    "    y = np.array(y).astype(int)\n",
    "\n",
    "    #We do a simple normalization of our data\n",
    "    X = X / 255\n",
    "\n",
    "    return train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03ccda",
   "metadata": {},
   "source": [
    "A simple test of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cf7e2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for epoch 1000 : 10.160055671537926\n",
      "Accuracy for epoch 1000 : 18.997912317327767\n",
      "Accuracy for epoch 1000 : 21.781489213639528\n",
      "Accuracy for epoch 1000 : 17.606123869171885\n",
      "Accuracy for epoch 1000 : 38.20459290187891\n",
      "Accuracy for epoch 1000 : 49.40848990953375\n",
      "Accuracy for epoch 1000 : 54.27974947807933\n",
      "Accuracy for epoch 1000 : 48.29505915100904\n",
      "Accuracy for epoch 1000 : 56.36743215031316\n",
      "Accuracy for epoch 1000 : 57.48086290883786\n",
      "Accuracy for epoch 2000 : 10.090466249130133\n",
      "Accuracy for epoch 2000 : 10.716771050800277\n",
      "Accuracy for epoch 2000 : 12.734864300626306\n",
      "Accuracy for epoch 2000 : 14.057063326374392\n",
      "Accuracy for epoch 2000 : 30.410577592205986\n",
      "Accuracy for epoch 2000 : 40.292275574112736\n",
      "Accuracy for epoch 2000 : 54.83646485734168\n",
      "Accuracy for epoch 2000 : 36.74321503131524\n",
      "Accuracy for epoch 2000 : 61.02992345163535\n",
      "Accuracy for epoch 2000 : 55.045233124565065\n",
      "Accuracy for epoch 2000 : 43.77174669450243\n",
      "Accuracy for epoch 2000 : 48.29505915100904\n",
      "Accuracy for epoch 2000 : 51.00904662491301\n",
      "Accuracy for epoch 2000 : 62.700069589422405\n",
      "Accuracy for epoch 2000 : 60.61238691718859\n",
      "Accuracy for epoch 2000 : 51.28740431454419\n",
      "Accuracy for epoch 2000 : 55.323590814196244\n",
      "Accuracy for epoch 2000 : 53.93180236604036\n",
      "Accuracy for epoch 2000 : 55.880306193458594\n",
      "Accuracy for epoch 2000 : 63.952679192762695\n",
      "Accuracy for epoch 3000 : 10.160055671537926\n",
      "Accuracy for epoch 3000 : 10.368823938761308\n",
      "Accuracy for epoch 3000 : 29.436325678496868\n",
      "Accuracy for epoch 3000 : 47.25121781489214\n",
      "Accuracy for epoch 3000 : 36.18649965205289\n",
      "Accuracy for epoch 3000 : 42.240779401530965\n",
      "Accuracy for epoch 3000 : 47.18162839248434\n",
      "Accuracy for epoch 3000 : 45.650661099512874\n",
      "Accuracy for epoch 3000 : 52.609603340292274\n",
      "Accuracy for epoch 3000 : 42.86708420320111\n",
      "Accuracy for epoch 3000 : 65.06610995128742\n",
      "Accuracy for epoch 3000 : 57.20250521920668\n",
      "Accuracy for epoch 3000 : 61.30828114126653\n",
      "Accuracy for epoch 3000 : 62.421711899791234\n",
      "Accuracy for epoch 3000 : 44.32846207376478\n",
      "Accuracy for epoch 3000 : 58.38552540013918\n",
      "Accuracy for epoch 3000 : 59.84690327070285\n",
      "Accuracy for epoch 3000 : 55.11482254697286\n",
      "Accuracy for epoch 3000 : 59.568545581071675\n",
      "Accuracy for epoch 3000 : 67.15379262352123\n",
      "Accuracy for epoch 3000 : 60.264439805149614\n",
      "Accuracy for epoch 3000 : 58.107167710508\n",
      "Accuracy for epoch 3000 : 55.67153792623522\n",
      "Accuracy for epoch 3000 : 62.21294363256785\n",
      "Accuracy for epoch 3000 : 60.33402922755741\n",
      "Accuracy for epoch 3000 : 65.90118302018094\n",
      "Accuracy for epoch 3000 : 63.81350034794712\n",
      "Accuracy for epoch 3000 : 66.66666666666666\n",
      "Accuracy for epoch 3000 : 63.88308977035491\n",
      "Accuracy for epoch 3000 : 68.75434933890048\n",
      "Accuracy for epoch 4000 : 10.090466249130133\n",
      "Accuracy for epoch 4000 : 10.368823938761308\n",
      "Accuracy for epoch 4000 : 10.716771050800277\n",
      "Accuracy for epoch 4000 : 21.64231036882394\n",
      "Accuracy for epoch 4000 : 44.676409185803756\n",
      "Accuracy for epoch 4000 : 51.0786360473208\n",
      "Accuracy for epoch 4000 : 55.81071677105081\n",
      "Accuracy for epoch 4000 : 56.854558107167705\n",
      "Accuracy for epoch 4000 : 50.59151009046625\n",
      "Accuracy for epoch 4000 : 58.31593597773138\n",
      "Accuracy for epoch 4000 : 48.15588030619346\n",
      "Accuracy for epoch 4000 : 57.62004175365344\n",
      "Accuracy for epoch 4000 : 53.58385525400139\n",
      "Accuracy for epoch 4000 : 60.05567153792624\n",
      "Accuracy for epoch 4000 : 58.107167710508\n",
      "Accuracy for epoch 4000 : 58.80306193458594\n",
      "Accuracy for epoch 4000 : 52.470424495476685\n",
      "Accuracy for epoch 4000 : 54.210160055671544\n",
      "Accuracy for epoch 4000 : 71.67710508002784\n",
      "Accuracy for epoch 4000 : 53.72303409881698\n",
      "Accuracy for epoch 4000 : 60.125260960334025\n",
      "Accuracy for epoch 4000 : 55.39318023660403\n",
      "Accuracy for epoch 4000 : 53.37508698677801\n",
      "Accuracy for epoch 4000 : 61.864996520528884\n",
      "Accuracy for epoch 4000 : 66.52748782185108\n",
      "Accuracy for epoch 4000 : 65.34446764091858\n",
      "Accuracy for epoch 4000 : 60.473208072373\n",
      "Accuracy for epoch 4000 : 61.16910229645094\n",
      "Accuracy for epoch 4000 : 55.323590814196244\n",
      "Accuracy for epoch 4000 : 54.90605427974948\n",
      "Accuracy for epoch 4000 : 62.07376478775226\n",
      "Accuracy for epoch 4000 : 60.33402922755741\n",
      "Accuracy for epoch 4000 : 64.43980514961726\n",
      "Accuracy for epoch 4000 : 59.011830201809325\n",
      "Accuracy for epoch 4000 : 60.75156576200418\n",
      "Accuracy for epoch 4000 : 64.23103688239388\n",
      "Accuracy for epoch 4000 : 58.73347251217815\n",
      "Accuracy for epoch 4000 : 64.71816283924844\n",
      "Accuracy for epoch 4000 : 66.2491301322199\n",
      "Accuracy for epoch 4000 : 59.707724425887264\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGwCAYAAABcnuQpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABRrUlEQVR4nO3deVhU9eIG8HdmgGGTQXZQQERFHUFFcavM0sTdym7pRUXplluZP8tCb7heQ7xlliUtN5XCJSstr5ZlrlkmIqsb7oqyiQjDvsx8f394nRxXUODMMO/neeZ55JwzZ945HZi3M+d7jkwIIUBERERkouRSByAiIiJ6GCwzREREZNJYZoiIiMikscwQERGRSWOZISIiIpPGMkNEREQmjWWGiIiITJqF1AEamk6nQ1ZWFpo1awaZTCZ1HCIiIqoFIQSKi4vh5eUFufzex16afJnJysqCt7e31DGIiIjoAWRmZqJly5b3XKbJl5lmzZoBuL4xHBwcJE5DREREtaHRaODt7a3/HL+XJl9mbny15ODgwDJDRERkYmpzioikJwBrtVpERUXBz88PNjY28Pf3x6JFi3Dr7aKOHz+OESNGQKVSwc7ODiEhIbh48aJEqYmIiMiYSHpkJiYmBrGxsYiLi4NarUZiYiImTpwIlUqF6dOnAwDOnDmDRx99FC+++CIWLFgABwcHHD16FNbW1lJGJyIiIiMhk/Ku2cOGDYO7uzu++OIL/bRRo0bBxsYG8fHxAIDRo0fD0tISX3311QO9hkajgUqlQlFREb9mIiIiMhF1+fyW9GumPn36YOfOnTh58iQAIDU1Ffv378fgwYMBXB9WvW3bNrRr1w6hoaFwc3NDz5498f333991nZWVldBoNAYPIiIiarokLTORkZEYPXo02rdvD0tLS3Tt2hUzZsxAWFgYACAvLw8lJSVYsmQJBg0ahF9++QXPPPMMnn32Wezdu/eO64yOjoZKpdI/OCybiIioaZP0a6YNGzZg1qxZ+Pe//w21Wo2UlBTMmDEDy5YtQ3h4OLKystCiRQuMGTMG69at0z9vxIgRsLOzw/r1629bZ2VlJSorK/U/3xjaxa+ZiIiITEddvmaS9ATgWbNm6Y/OAEBgYCAuXLiA6OhohIeHw8XFBRYWFujYsaPB8zp06ID9+/ffcZ1KpRJKpbLBsxMREZFxkPRrprKystsuUaxQKKDT6QAAVlZWCAkJQUZGhsEyJ0+ehK+vb6PlJCIiIuMl6ZGZ4cOHY/HixfDx8YFarUZycjKWLVuGiIgI/TKzZs3CCy+8gL59++KJJ57A9u3b8d///hd79uyRLjgREREZDUnPmSkuLkZUVBQ2b96MvLw8eHl5YcyYMZg7dy6srKz0y61atQrR0dG4dOkSAgICsGDBAowcObJWr8Gh2URERKanLp/fkpaZxsAyQ0REZHpM5jozRERERA+LZYaIiIgeiBACR7OKcLWk8v4LN6Amf9dsIiIiqj9CCJzIKca2tGxsS8/GufxSvD20A/7xWGvJMrHMEBER0T0JIZCRW4wf07KxNT0bZ6+U6ucpLeQoLKuWMB3LDBEREd3FydxibE3Lxra0LJy5qcBYWcjRr50rhgZ5on8Hd9grpa0TLDNERESkdyq3GNvSs7EtLRun8kr0060Ucjwe4IphQZ54sr0bmllbSpjSEMsMERGRmTudV/K/c2CycDLXsMD0beeiPwLjYEQF5mYsM0RERGbozJUS/Pi/k3hP5BTrp1sqZOjb9vpXSAM6Gm+BuRnLDBERkZk4e6UEP6ZnY2uaYYGxkMvwWFsXDA3ywlMd3aGyMf4CczOWGSIioibsfH6p/hyYY9ka/XQLuQyPtnXBkEBPhHb0gMrWtArMzVhmiIiImpgLV/8qMEez/iowCrkMj7RxwbBATwxUu8PR1uoeazEdLDNERERNQGZBmb7ApF8u0k9XyGXo4++MoYGeCFV7oLld0ygwN2OZISIiMlGZBWX4Mf36Sbxpl/4qMHIZ0Mf/+iikULUHnJpggbkZywwREZEJuXStDD+l52BrejZSMwv10+UyoFdrZ32BcbFXSheykbHMEBERGbmswnL9KKSUWwpMT7/rBWZQJ/MqMDdjmSEiIjJC2UXl+DE9B9vSspB0sVA/XSYDerRywrAgT4R28oBbM2vpQhoJlhkiIiIjkVNUgR/Ts/FjejYSL1zTT5fJgJD/FZhBag+4ObDA3IxlhoiISEK5mgr89L+TeA+dv2YwL6RVcwwN9MTgQE+4s8DcFcsMERFRI8srrsD2IznYmpaNQ+cLIMRf87r7NseQQE8MCfSEh4oFpjZYZoiIiBrBleJKbD9y/STehFsKTLCPI4YGeWFIoAc8VTbShTRRLDNEREQNJL+kEtuP5GBbWjYOnrsK3U0FpquPo/4rpBaOLDAPg2WGiIioHl0tqcT2o9cLzJ9nDQtMZ29HDAv0xOBAD7RsbitdyCaGZYaIiOghFZRW4ef/FZgDZ69Ce1OD6dxSpT8HxtuJBaYhsMwQERE9gGs3Ckx6Nv44Y1hgAluoMDTIE0NZYBoFywwREVEtFZZV4Zejudiano3fT+cbFBi1l4O+wPg620mY0vywzBAREd1DUVk1fj6Wgx/Ts7H/VD5qbiowHT3/KjCtXFhgpMIyQ0REdIui8mrsOJaLbWlZ2H86H9XavwpMe49mGBZ0/RyY1q72EqakG1hmiIiIAGgqqvHrsVxsS8vGvlNXbiswQwM9MSTIE/4sMEaHZYaIiMxWcUU1fj3+vwJzMh9VWp1+Xjt3ewwN9MLQIA+0cWsmYUq6H5YZIiIyKyWVNdh5PBdb07Kx9+QVVNX8VWDauNlj2P/OgWnrzgJjKlhmiIioybtRYLalZWPPLQXG39UOQ4O8MCzIE+1YYEwSywwRETVJpZU12HUiD9vSsrE7Iw+VNxWY1i5210/iDfJEgHszyGQyCZPSw2KZISKiJqOsyrDAVFT/VWD8XOwwNNATQ4M80d6DBaYpYZkhIiKTVl6lxe6M6wVm54lcgwLj62yrH0bd0dOBBaaJYpkhIiKTU16lxZ6MPGxNz8au43kor9bq5/k42eovZKf2YoExBywzRERkEiqqtdiTcQXb0rOx83guyqr+KjDeTjbXh1EHeqJTCxYYc8MyQ0RERquiWou9J69c/wrpeC5KbyowLRxtrg+jDvJEYAsVC4wZY5khIiKjUlGtxW+n8rEtLQu/Hs9DSWWNfl4LRxsMCfTA0CAvdG7JAkPXscwQEZHkKmu0+O1kPn5Mz8aOY7kovqnAeKqs9aOQung7ssDQbVhmiIhIElU1Ouw/fQVb07Kx46hhgfFwsMaQ/xWYrt6OkMtZYOjuWGaIiKjRVNXo8PvpfGxLz8YvR3OgqfirwLg7KDEk0BPDgjzR1bs5CwzVGssMERE1qGrt/wpMWjZ+vqXAuDVT6o/AdPNhgaEHwzJDRET1rlqrw4EzV68XmGM5KCyr1s9zbabEkE4eGBLoie6tnKBggaGHxDJDRET1okarw4Gz1wvM9qOGBcbF3gqDO10/AhPCAkP1jGWGiIgeWI1Wh4PnCrD1f18hFZRW6ec521lhcOD1IzA9/ZxZYKjBsMwQEVGd1Gh1SDhXgK3p2fj5SA6u3lRgnOysMKiTB4YFeqKHnxMsFHIJk5K5YJkhIqL70uoEEs4VYFt6FrYfyUF+yV8FprmtJQZ1un4vpF6tWWCo8bHMEBHRHWl1AofOF2BbWjZ+OpKD/JJK/TxHW0sMUntgaJAnerV2hiULDEmIZYaIiPR0OoHEC9ewLS0LPx7JwZXivwqMysYSoWp3DA3yQh9/FhgyHiwzRERmTqcTOHzxGralZePH9Gzk3VRgHKwtEPq/IzB9/F1gZcECQ8ZH0jKj1Woxf/58xMfHIycnB15eXpgwYQLefvtt/b03JkyYgLi4OIPnhYaGYvv27VJEJiJqEnQ6geTMa9ialo2f0nOQo6nQz2tmbYGBHT0wLMgTj7RhgSHjJ2mZiYmJQWxsLOLi4qBWq5GYmIiJEydCpVJh+vTp+uUGDRqE1atX639WKpVSxCUiMmnXC0whfky/fgQmu+imAqO0wFNqd32BUVooJExKVDeSlpk//vgDI0eOxNChQwEArVq1wvr165GQkGCwnFKphIeHR63WWVlZicrKvw6RajSa+gtMRGSi1h28iI92nULWTQXGXmmBpzq6Y2igJx5rxwJDpkvSY4d9+vTBzp07cfLkSQBAamoq9u/fj8GDBxsst2fPHri5uSEgIABTpkzB1atX77rO6OhoqFQq/cPb27tB3wMRkbHbeCgTczanI6uoAnZWCjzdxQufj++OxLcH4P0XumBAR3cWGTJpMiGEkOrFdTod5syZg6VLl0KhUECr1WLx4sWYPXu2fpkNGzbA1tYWfn5+OHPmDObMmQN7e3scOHAACsXtv3x3OjLj7e2NoqIiODg4NMr7IiIyFrtO5OKlLw9DqxN46TE/vD4wANaWLC5k/DQaDVQqVa0+vyX9mmnjxo1Yu3Yt1q1bB7VajZSUFMyYMQNeXl4IDw8HAIwePVq/fGBgIIKCguDv7489e/agf//+t61TqVTynBoiIgDJF69h6tokaHUCo4JbYs6QDvrBFURNiaRlZtasWYiMjNQXlsDAQFy4cAHR0dH6MnOr1q1bw8XFBadPn75jmSEiIuDslRJErDmEimodHm/niiWjAllkqMmS9JyZsrIyyOWGERQKBXQ63V2fc+nSJVy9ehWenp4NHY+IyCTlFVdg/KoEXCurRlBLFVaGBfMCd9SkSXpkZvjw4Vi8eDF8fHygVquRnJyMZcuWISIiAgBQUlKCBQsWYNSoUfDw8MCZM2fw5ptvok2bNggNDZUyOhGRUSquqMbE1Ydw6Vo5fJ1tsWpCCOyUvD4qNW2S7uErVqxAVFQUpk6diry8PHh5eWHSpEmYO3cugOtHadLS0hAXF4fCwkJ4eXlh4MCBWLRoEc+LISK6RVWNDlPik3A0SwNnOyt8GdEDLvb8W0lNn6SjmRpDXc6GJiIyVTqdwMyNKfg+JQu2VgpseLkXglo6Sh2L6IHV5fObX6ISETUBMdtP4PuULFjIZVgZFswiQ2aFZYaIyMSt2n8On+47CwBYMioI/QLcJE5E1LhYZoiITNjWtCws2nYMADArNADPdWspcSKixscyQ0Rkov44k4+ZX6dCCGB8b19M7ecvdSQiSbDMEBGZoOPZGkz68jCqtDoM7uSBecPVvCgemS2WGSIiE3O5sBwTVieguLIGPVo54f0XukAhZ5Eh88UyQ0RkQgrLqhC+KgG5mkq0c7fH5+O788aRZPZYZoiITERFtRYvxiXidF4JPByssWZiD6hsLaWORSQ5lhkiIhOg1QlMX5+MwxeuwcHaAnERPeDlaCN1LCKjwDJDRGTkhBCY+8MR/HIsF1YWcnw+vjsCPJpJHYvIaLDMEBEZuY92ncbagxchkwEfvNAFPVs7Sx2JyKiwzBARGbGNhzLx3o6TAID5w9UYHOgpcSIi48MyQ0RkpHadyMXszekAgCn9/BHep5W0gYiMFMsMEZERSsksxLS1ydDqBJ4NboE3QwOkjkRktFhmiIiMzNkrJYhYcwjl1Vr0beeKmFFBvLov0T2wzBARGZG84gqEr05AQWkVAluoEBsWDEsF/1QT3Qt/Q4iIjERJZQ0i1hxCZkE5fJ1tsWpCCOyUFlLHIjJ6LDNEREagqkaHKfGHceSyBs52Voib2AOuzZRSxyIyCSwzREQS0+kE3vouDb+dyoeNpQKrJoSglYud1LGITAbLDBGRxGJ+PoHNyZdhIZchdmwwOns7Sh2JyKSwzBARSWjV/nP4dO9ZAMCSUUHoF+AmcSIi08MyQ0Qkka1pWVi07RgAYFZoAJ7r1lLiRESmiWWGiEgCf5zJx8yvUyEEML63L6b285c6EpHJYpkhImpkx7M1mPTlYVRpdRjcyQPzhqt5UTyih8AyQ0TUiC4XlmPC6gQUV9agRysnvP9CFyjkLDJED4NlhoiokRSWVSF8VQJyNZVo526Pz8d3h7WlQupYRCaPZYaIqBFUVGvxYlwiTueVwMPBGmsm9oDK1lLqWERNAssMEVED0+oEpq9PxuEL1+BgbYG4iB7wcrSROhZRk8EyQ0TUgIQQmPvDEfxyLBdWFnJ8Pr47AjyaSR2LqElhmSEiakAf7TqNtQcvQiYDPnihC3q2dpY6ElGTwzJDRNRANh7KxHs7TgIA5g9XY3Cgp8SJiJomlhkiogaw60QuZm9OBwBM6eeP8D6tpA1E1ISxzBAR1bOUzEJMW5sMrU7g2eAWeDM0QOpIRE0aywwRUT06e6UEEWsOobxai77tXBEzKohX9yVqYCwzRET1JK+4AuGrE1BQWoXAFirEhgXDUsE/s0QNjb9lRET1oKSyBhFrDiGzoBy+zrZYNSEEdkoLqWMRmQWWGSKih1RVo8OU+MM4clkDZzsrxE3sAddmSqljEZkNlhkiooeg0wm89V0afjuVDxtLBVZNCEErFzupYxGZFZYZIqKHEPPzCWxOvgyFXIaVY4PR2dtR6khEZodlhojoAa3+/Rw+3XsWALDk2UA8EeAmcSIi88QyQ0T0ALamZWHh1mMAgFmhAfhbd2+JExGZL5YZIqI6OnDmKmZ+nQohgHG9fDG1n7/UkYjMGssMEVEdnMjR4OWvElGl1WGQ2gPzR6h5UTwiibHMEBHV0uXCcoSvSkBxRQ1CWjXH8tFdoJCzyBBJjWWGiKgWCsuqEL4qAbmaSrR1s8d/xofA2lIhdSwiAssMEdF9VVRr8Y+4RJzOK4GHgzXiInpAZWspdSwi+h+WGSKie9DqBKavT0bihWtwsLZAXEQPeDnaSB2LiG7CMkNEdBdCCMz94Qh+OZYLKws5Ph/fHQEezaSORUS3YJkhIrqLj3adxtqDFyGTAR+80AU9WztLHYmI7kDSMqPVahEVFQU/Pz/Y2NjA398fixYtghDijstPnjwZMpkMy5cvb9ygRGR2Nh7KxHs7TgIA5g9XY3Cgp8SJiOhuJL0/fUxMDGJjYxEXFwe1Wo3ExERMnDgRKpUK06dPN1h28+bN+PPPP+Hl5SVRWiIyF7tO5GL25nQAwJR+/gjv00raQER0T5KWmT/++AMjR47E0KFDAQCtWrXC+vXrkZCQYLDc5cuX8eqrr+Lnn3/WL0tE1BBSMgsxbW0ytDqBZ4Nb4M3QAKkjEdF9SPo1U58+fbBz506cPHn9UG5qair279+PwYMH65fR6XQYN24cZs2aBbVafd91VlZWQqPRGDyIiGrj7JUSRKw5hPJqLfq2c0XMqCBe3ZfIBEh6ZCYyMhIajQbt27eHQqGAVqvF4sWLERYWpl8mJiYGFhYWt33tdDfR0dFYsGBBQ0UmoiYqr7gC4asTUFBahcAWKsSGBcNSwTESRKZA0t/UjRs3Yu3atVi3bh2SkpIQFxeHd999F3FxcQCAw4cP44MPPsCaNWtq/X9Hs2fPRlFRkf6RmZnZkG+BiJqAksoaRKw5hMyCcvg622LVhBDYKSX9fz0iqgOZuNvQoUbg7e2NyMhITJs2TT/tX//6F+Lj43HixAksX74cM2fOhFz+V+fSarWQy+Xw9vbG+fPn7/saGo0GKpUKRUVFcHBwaIi3QUQmrKpGhxfjDuG3U/lwtrPCd1P6oJWLndSxiMxeXT6/Jf1fj7KyMoOiAgAKhQI6nQ4AMG7cOAwYMMBgfmhoKMaNG4eJEyc2Wk4iapp0OoG3vkvDb6fyYWOpwKoJISwyRCZI0jIzfPhwLF68GD4+PlCr1UhOTsayZcsQEREBAHB2doazs+FFqiwtLeHh4YGAAI4wIKKHE/PzCWxOvgyFXIaVY4PR2dtR6khE9AAkLTMrVqxAVFQUpk6diry8PHh5eWHSpEmYO3eulLGIyAys/v0cPt17FgCw5NlAPBHgJnEiInpQkp4z0xh4zgwR3WprWhZeXZ8MIYBZoQGY9kQbqSMR0S3q8vnNcYdEZFYOnLmKmV+nQghgXC9fTO3nL3UkInpILDNEZDZO5Gjw8leJqNLqMEjtgfkj1LwoHlETwDJDRGbhcmE5wlcloLiiBiGtmmP56C5QyFlkiJoClhkiavIKy6oQvioBuZpKtHWzx3/Gh8DaUiF1LCKqJywzRNSkVVRr8Y+4RJzOK4GHgzXiInpAZWspdSwiqkcsM0TUZGl1AtPXJyPxwjU0s7ZAXEQPeDnaSB2LiOoZywwRNUlCCMzbcgS/HMuFlUKOz8d3R4BHM6ljEVEDYJkhoibp492nEf/nRchkwPLRXdCrtfP9n0REJollhoianI2JmXj3l5MAgHnDOmJIoKfEiYioIbHMEFGTsvtEHmZvSgcATH7cHxMe8ZM4ERE1NJYZImoyUjILMXVtErQ6gWe7tsBbg3hDWiJzwDJDRE3CufxSRKw5hPJqLfq2c0XMc0G8ui+RmWCZISKTd6W4EuNXHURBaRUCW6gQGxYMSwX/vBGZC/62E5FJK6mswcQ1CcgsKIevsy1WTQiBndJC6lhE1IhYZojIZFXV6DAl/jCOXNbA2c4KcRN7wLWZUupYRNTIWGaIyCTpdAJvfZeG307lw8ZSgVUTQtDKxU7qWEQkAZYZIjJJMT+fwObky1DIZVg5NhidvR2ljkREEmGZISKTs/r3c/h071kAwJJnA/FEgJvEiYhISiwzRGRStqZlYeHWYwCAWaEB+Ft3b4kTEZHUWGaIyGQcOHMVM79OhRDAuF6+mNrPX+pIRGQEWGaIyCScyNHg5a8SUaXVYZDaA/NHqHlRPCICwDJDRCbgcmE5wlcloLiiBiGtmmP56C5QyFlkiOi6OpeZVq1aYeHChbh48WJD5CEiMlBYVoXwVQnI1VSirZs9/jM+BNaWCqljEZERqXOZmTFjBjZt2oTWrVvjqaeewoYNG1BZWdkQ2YjIzFVUa/GPuEScziuBh4M14iJ6QGVrKXUsIjIyD1RmUlJSkJCQgA4dOuDVV1+Fp6cnXnnlFSQlJTVERiIyQ1qdwPT1yUi8cA3NrC0QF9EDXo42UsciIiP0wOfMBAcH48MPP0RWVhbmzZuH//znPwgJCUGXLl2watUqCCHqMycRmREhBOZtOYJfjuXCSiHH5+O7I8CjmdSxiMhIPfDd2Kqrq7F582asXr0aO3bsQK9evfDiiy/i0qVLmDNnDn799VesW7euPrMSkZn4ePdpxP95ETIZsHx0F/Rq7Sx1JCIyYnUuM0lJSVi9ejXWr18PuVyO8ePH4/3330f79u31yzzzzDMICQmp16BEZB42Jmbi3V9OAgDmDeuIIYGeEiciImNX5zITEhKCp556CrGxsXj66adhaXn7yXh+fn4YPXp0vQQkIvOx+0QeZm9KBwBMftwfEx7xkzgREZmCOpeZs2fPwtfX957L2NnZYfXq1Q8ciojMT0pmIaauTYJWJ/Bs1xZ4a1CA1JGIyETU+QTgvLw8HDx48LbpBw8eRGJiYr2EIiLzci6/FBFrDqG8WovH2rog5rkgXt2XiGqtzmVm2rRpyMzMvG365cuXMW3atHoJRUTm40pxJcavOoiC0ioEtlAhdmw3WCp4cXIiqr06/8U4duwYgoODb5vetWtXHDt2rF5CEZF5KKmswcQ1CcgsKIePky1WTQiBvfKBB1kSkZmqc5lRKpXIzc29bXp2djYsLPhHiIhqp6pGhynxh3HksgZOdlaIi+gB12ZKqWMRkQmqc5kZOHAgZs+ejaKiIv20wsJCzJkzB0899VS9hiOipkkIgcjv0vDbqXzYWCqwakII/FzspI5FRCaqzodS3n33XfTt2xe+vr7o2rUrACAlJQXu7u746quv6j0gETU9MdszsCn5MhRyGVaODUYXb0epIxGRCatzmWnRogXS0tKwdu1apKamwsbGBhMnTsSYMWPueM0ZIqKbrfn9HD7ZewYAsOTZQDwR4CZxIiIydQ90koudnR1efvnl+s5CRE3ctrRsLNh6faDArNAA/K27t8SJiKgpeOAzdo8dO4aLFy+iqqrKYPqIESMeOhQRNT1/nr2K//s6BUIA43r5Ymo/f6kjEVET8UBXAH7mmWeQnp4OmUymvzv2jQtcabXa+k1IRCbvRI4GL32ZiCqtDoPUHpg/Qs2L4hFRvanzaKbXXnsNfn5+yMvLg62tLY4ePYp9+/ahe/fu2LNnTwNEJCJTllVYjgmrDqG4ogYhrZpj+eguUMhZZIio/tT5yMyBAwewa9cuuLi4QC6XQy6X49FHH0V0dDSmT5+O5OTkhshJRCaosKwK4asSkKOpQFs3e/xnfAisLRVSxyKiJqbOR2a0Wi2aNWsGAHBxcUFWVhYAwNfXFxkZGfWbjohMVkW1Fi99mYhTeSXwcLBGXEQPqGw54pGI6l+dj8x06tQJqamp8PPzQ8+ePbF06VJYWVnhs88+Q+vWrRsiIxGZGK1OYPr6ZBw6fw3NrC0QF9EDXo42UscioiaqzmXm7bffRmlpKQBg4cKFGDZsGB577DE4Ozvj66+/rveARGRahBCYt+UIfjmWCyuFHJ+P744Aj2ZSxyKiJkwmbgxHeggFBQVo3ry5UY5O0Gg0UKlUKCoqgoODg9RxiJq8j3adwru/nIRMBnz892AMCfSUOhIRmaC6fH7X6ZyZ6upqWFhY4MiRIwbTnZycjLLIEFHj2piYiXd/OQkAmDesI4sMETWKOpUZS0tL+Pj48FoyRHSb3SfyMHtTOgBg8uP+mPCIn8SJiMhc1Hk00z//+U/MmTMHBQUFDZGHiExQSmYhpq5NglYn8GzXFnhrUIDUkYjIjNT5BOCPPvoIp0+fhpeXF3x9fWFnZ2cwPykpqd7CEZHxO5dfiog1h1BercVjbV0Q81wQv3YmokZV5zLz9NNP19uLa7VazJ8/H/Hx8cjJyYGXlxcmTJiAt99+W//HcP78+diwYQMyMzNhZWWFbt26YfHixejZs2e95SCiB3OluBLjVx1EQWkVAluoEDu2GywVdT7gS0T0UOpcZubNm1dvLx4TE4PY2FjExcVBrVYjMTEREydOhEqlwvTp0wEA7dq1w0cffYTWrVujvLwc77//PgYOHIjTp0/D1dW13rIQUd2UVNZg4poEZBaUw8fJFqsmhMBe+cD3riUiemD1MjT7QQ0bNgzu7u744osv9NNGjRoFGxsbxMfH3/E5N4Zq/frrr+jfv/9t8ysrK1FZWWmwvLe3N4dmE9WjqhodXow7hN9O5cPJzgrfTekDPxe7+z+RiKiWGmxoNgDI5XIoFIq7PuqiT58+2LlzJ06evD6UMzU1Ffv378fgwYPvuHxVVRU+++wzqFQqdO7c+Y7LREdHQ6VS6R/e3t51e4NEdE9CCER+l4bfTuXDxlKBVRNCWGSISFJ1Pia8efNmg5+rq6uRnJyMuLg4LFiwoE7rioyMhEajQfv27aFQKKDVarF48WKEhYUZLLd161aMHj0aZWVl8PT0xI4dO+Di4nLHdc6ePRszZ87U/3zjyAwR1Y+Y7RnYlHwZCrkMK8cGo4u3o9SRiMjM1bnMjBw58rZpzz33HNRqNb7++mu8+OKLtV7Xxo0bsXbtWqxbtw5qtRopKSmYMWMGvLy8EB4erl/uiSeeQEpKCvLz8/H555/j+eefx8GDB+Hm5nbbOpVKJZRKZV3fFhHVwprfz+GTvWcAAEueDcQTAbf/DhIRNbZ6O2fm7NmzCAoKQklJSa2f4+3tjcjISEybNk0/7V//+hfi4+Nx4sSJuz6vbdu2iIiIwOzZs+/7GrydAVH92JaWjVfWJ0EI4I2B7fDKk22ljkRETViDnjNzJ+Xl5fjwww/RokWLOj2vrKwMcrlhBIVCAZ1Od8/n6XQ6g5N8iahh/Xn2Kv7v6xQIAYzr5YtpT7SROhIRkV6dv2a69YaSQggUFxfD1tb2riOQ7mb48OFYvHgxfHx8oFarkZycjGXLliEiIgIAUFpaisWLF2PEiBHw9PREfn4+Pv74Y1y+fBl/+9vf6hqdiB7AiRwNXvoyEVVaHULV7pg/Qs2L4hGRUalzmXn//fcN/pDJ5XK4urqiZ8+eaN68eZ3WtWLFCkRFRWHq1KnIy8uDl5cXJk2ahLlz5wK4fpTmxIkTiIuLQ35+PpydnRESEoLffvsNarW6rtGJqI6yCssxYdUhFFfUoLtvc3wwuisUchYZIjIukl5npjHwnBmiB1NYVoW/fXIAp/JK0MbNHt9O7g1HWyupYxGRmWjQc2ZWr16Nb7755rbp33zzDeLi4uq6OiIyQhXVWrz0ZSJO5ZXAw8EacRE9WGSIyGjVucxER0ff8Rovbm5ueOedd+olFBFJR6sTeG1DMg6dv4Zm1hZYExGCFo42UsciIrqrOpeZixcvws/P77bpvr6+uHjxYr2EIiJpCCEwf8tR/Hw0F1YKOT4f3x3tPfj1LBEZtzqXGTc3N6Slpd02PTU1Fc7OzvUSioiksXLPGXz15wXIZMDy0V3QqzV/p4nI+NW5zIwZMwbTp0/H7t27odVqodVqsWvXLrz22msYPXp0Q2QkokbwTWIm/v1zBgBg3rCOGBLoKXEiIqLaqfPQ7EWLFuH8+fPo378/LCyuP12n02H8+PE8Z4bIRO3OyEPkpnQAwOTH/THhkdu/SiYiMlYPPDT71KlTSElJgY2NDQIDA+Hr61vf2eoFh2YT3VtqZiFGf/Ynyqu1eLZrC7z3fGdeFI+IJFeXz+86H5m5oW3btmjblvdmITJl5/NLEbHmEMqrtXisrQtingtikSEik1Pnc2ZGjRqFmJiY26YvXbqUtxggMiFXiisxflUCrpZWIbCFCrFju8FSUS+3ayMialR1/su1b98+DBky5LbpgwcPxr59++olFBE1rJLKGkxck4CLBWXwcbLFqgkhsFc+8IFaIiJJ1bnMlJSUwMrq9iuBWlpaQqPR1EsoImo4VTU6TIk/jCOXNXCys0JcRA+4NlNKHYuI6IHVucwEBgbi66+/vm36hg0b0LFjx3oJRUQNQwiByO/S8NupfNhYKrBqQgj8XOykjkVE9FDqfFw5KioKzz77LM6cOYMnn3wSALBz506sW7cO3377bb0HJKL6E7M9A5uSL0Mhl2Hl2GB08XaUOhIR0UOrc5kZPnw4vv/+e7zzzjv49ttvYWNjg86dO2PXrl1wcnJqiIxEVA/W/H4On+w9AwBY8mwgnghwkzgREVH9eODrzNyg0Wiwfv16fPHFFzh8+DC0Wm19ZasXvM4MEbAtLRuvrE+CEMAbA9vhlSd5WQUiMm51+fx+4HGY+/btQ3h4OLy8vPDee+/hySefxJ9//vmgqyOiBvLn2av4v69TIAQwrpcvpj3RRupIRET1qk5fM+Xk5GDNmjX44osvoNFo8Pzzz6OyshLff/89T/4lMkIncjR46ctEVGl1CFW7Y/4INS+KR0RNTq2PzAwfPhwBAQFIS0vD8uXLkZWVhRUrVjRkNiJ6CFmF5Ziw6hCKK2rQ3bc5PhjdFQo5iwwRNT21PjLz008/Yfr06ZgyZQpvY0Bk5ArLqhC+KgE5mgq0cbPHf8K7w9pSIXUsIqIGUesjM/v370dxcTG6deuGnj174qOPPkJ+fn5DZiOiB1BRrcVLXybiVF4JPBysERfRA462t1/okoioqah1menVqxc+//xzZGdnY9KkSdiwYQO8vLyg0+mwY8cOFBcXN2ROIqoFrU7gtQ3JOHT+GppZW2BNRAhaONpIHYuIqEE91NDsjIwMfPHFF/jqq69QWFiIp556Clu2bKnPfA+NQ7PJXAghMPeHo/jqzwuwUsgRF9EDvf2dpY5FRPRAGmVoNgAEBARg6dKluHTpEtavX/8wqyKih7Ryzxl89ecFyGTA+y90YZEhIrPx0BfNM3Y8MkPm4JvETMz6Ng0AMG94R0x8xE/iRERED6fRjswQkfR2Z+QhclM6AGDS461ZZIjI7LDMEJmw1MxCTI1PglYn8EzXFngrtL3UkYiIGh3LDJGJOp9fiog1h1BercVjbV0QMyoIcl4Uj4jMEMsMkQm6UlyJ8asScLW0Cp1aOCB2bDdYWfDXmYjME//6EZmY0soaRKw5hIsFZfBxssXqCT1gr6zTbdaIiJoUlhkiE1Kt1WHK2iSkXy6Ck50V4iJ6wLWZUupYRESSYpkhMhFCCLz1XRr2nbwCG0sFVk0IgZ+LndSxiIgkxzJDZCKW/pyBTUmXoZDLsHJsMLp4O0odiYjIKLDMEJmAuD/OI3bPGQDAkmcD8USAm8SJiIiMB8sMkZH7MT0b8/97FADwxsB2+Ft3b4kTEREZF5YZIiN28OxVzPg6BUIA43r5YtoTbaSORERkdFhmiIxURk4x/vFlIqpqdAhVu2P+CDVkMl4Uj4joViwzREYoq7Ac4asSUFxRg+6+zfHB6K5Q8Oq+RER3xDJDZGQKy6oQvioBOZoKtHGzx3/Cu8PaUiF1LCIio8UyQ2REisqq8Y+4RJzKK4GHgzXiInrA0dZK6lhEREaN10AnMhJHs4owJT4JFwvK0MzaAmsiQtDC0UbqWERERo9lhsgIbEq6hNmb0lFZo0PL5jb4ZGw3tPdwkDoWEZFJYJkhklBVjQ6Lth7DV39eAAA83s4Vy1/oguZ2/GqJiKi2WGaIJJJdVI6pa5OQfLEQADC9f1u81r8tRy0REdURywyRBA6cuYpX1ychv6QKDtYWeP+FLujfwV3qWEREJollhqgRCSHw+W9nEbM9A1qdQHuPZvh0XDf4OvPu10RED4plhqiRlFTW4M1vU/Fjeg4A4JmuLfDOM4GwseI1ZIiIHgbLDFEjOJ1XgklfJeLMlVJYKmSIGtYR43r58vYERET1gGWGqIH9lJ6NN75JRWmVFu4OSqwM64Zuvs2ljkVE1GSwzBA1kBqtDv/+OQOf7jsLAOjp54SP/h4M12ZKiZMRETUtkt7OQKvVIioqCn5+frCxsYG/vz8WLVoEIQQAoLq6Gm+99RYCAwNhZ2cHLy8vjB8/HllZWVLGJrqv/JJKjP3ioL7IvPSYH9b+oyeLDBFRA5D0yExMTAxiY2MRFxcHtVqNxMRETJw4ESqVCtOnT0dZWRmSkpIQFRWFzp0749q1a3jttdcwYsQIJCYmShmd6K6SLl7D1Pgk5GgqYGulwL+f64yhQZ5SxyIiarJk4sZhEAkMGzYM7u7u+OKLL/TTRo0aBRsbG8THx9/xOYcOHUKPHj1w4cIF+Pj43Pc1NBoNVCoVioqK4ODAy8NTwxFCIP7gRSz871FUawVau9rh07Hd0Na9mdTRiIhMTl0+vyX9mqlPnz7YuXMnTp48CQBITU3F/v37MXjw4Ls+p6ioCDKZDI6OjnecX1lZCY1GY/AgamjlVVq8/k0qor4/gmqtwOBOHvhh2iMsMkREjUDSr5kiIyOh0WjQvn17KBQKaLVaLF68GGFhYXdcvqKiAm+99RbGjBlz15YWHR2NBQsWNGRsIgMXr5ZhUvxhHM/WQC4D3hrUHi/3bc1h10REjUTSIzMbN27E2rVrsW7dOiQlJSEuLg7vvvsu4uLiblu2uroazz//PIQQiI2Nves6Z8+ejaKiIv0jMzOzId8CmbndJ/IwbMVvOJ6tgbOdFeJf7IlJj/uzyBARNSJJj8zMmjULkZGRGD16NAAgMDAQFy5cQHR0NMLDw/XL3SgyFy5cwK5du+753ZlSqYRSyREj1LB0OoEPdp7Ch7tOQQigs7cjYsOC4eVoI3U0IiKzI2mZKSsrg1xueHBIoVBAp9Ppf75RZE6dOoXdu3fD2dm5sWMSGSgsq8L/fZ2C3RlXAABje/kgalhHKC14WwIiIilIWmaGDx+OxYsXw8fHB2q1GsnJyVi2bBkiIiIAXC8yzz33HJKSkrB161ZotVrk5Fy/r42TkxOsrKykjE9m6MjlIkxZexiZBeVQWsix+JlAPNetpdSxiIjMmqRDs4uLixEVFYXNmzcjLy8PXl5eGDNmDObOnQsrKyucP38efn5+d3zu7t270a9fv/u+BodmU3359vAl/HNzOiprdPB2skFsWDd0aqGSOhYRUZNUl89vSctMY2CZoYdVWaPFoq3HEP/nRQBAvwBXLH+hCxxteWSQiKih1OXzm/dmIrqH7KJyTIlPQkpmIQDgtf5t8Vr/tpDLOVqJiMhYsMwQ3cUfZ/Lx6rpkXC2tgoO1BZaP7oIn27tLHYuIiG7BMkN0CyEEPtt3FjHbT0AngA6eDvh0bDf4ONtKHY2IiO6AZYboJsUV1Zj1TRq2H70+au7Zri2w+JlA2Fhx2DURkbFimSH6n9N5xZj01WGcuVIKS4UMc4erMbanD6/mS0Rk5FhmiABsS8vGm9+morRKCw8Ha6wcG4xgn+ZSxyIiolpgmSGzVqPVYenPGfhs31kAQK/WTvjo78FwsectMYiITAXLDJmtK8WVeHV9Ev48WwAAmNS3NWaFBsBCIen9V4mIqI5YZsgsHb5wDVPXHkauphJ2Vgr8+2+dMSTQU+pYRET0AFhmyKwIIRD/5wUs3HoM1VoBf1c7fDquG9q4NZM6GhERPSCWGTIb5VVa/HNzOjYlXwYADAn0wNLnOsNeyV8DIiJTxr/iZBYuXC3F5PgkHM/WQC4DIge3x0uPteawayKiJoBlhpq8XSdyMWNDCjQVNXC2s8KKv3dFH38XqWMREVE9YZmhJkurE/hg5yl8uPMUAKCrjyNWhgXDU2UjcTIiIqpPLDPUJBWWVeG1DSnYe/IKAGBcL1+8PawDlBa8LQERUVPDMkNNzpHLRZgcfxiXrpVDaSHHO88EYlS3llLHIiKiBsIyQ03Kt4cv4Z+b01FZo4OPky1ixwZD7aWSOhYRETUglhlqEiprtFj432NYe/AiAOCJAFcsf6ErVLaWEicjIqKGxjJDJi+rsBxT1iYhNbMQMhnwWv+2mP5kW8jlHHZNRGQOWGbIpP1xOh+vrE9GQWkVHKwt8MHorniivZvUsYiIqBGxzJBJEkLg031nsXT7CegE0NHTAZ+M7QYfZ1upoxERUSNjmSGTU1xRjVnfpGH70RwAwKjgllj8TCdYW3LYNRGROWKZIZNyKrcYk+IP4+yVUlgqZJg3XI2wnj68LQERkRljmSGTsTUtC29+m4ayKi08HKwROzYYXX2aSx2LiIgkxjJDRq9Gq8OSn07gP/vPAQB6t3bGir93hYu9UuJkRERkDFhmyKhdKa7EK+uScPBcAQBg0uOtMWtgACwUcomTERGRsWCZIaN1+EIBpq5NQq6mEnZWCrz7t84YHOgpdSwiIjIyLDNkdIQQ+OrPC1i09RiqtQJt3OzxydhuaONmL3U0IiIyQiwzZFTKq7SYszkdm5MvAwCGBnoi5rkg2Cu5qxIR0Z3xE4KMxvn8UkyOP4wTOcVQyGWIHNQe/3jMj8OuiYjonlhmyCjsPJ6LGV+noLiiBi72VlgxJhi9/Z2ljkVERCaAZYYkpdUJfPDrSXy46zQAINjHESvDusFDZS1xMiIiMhUsMySZwrIqvLYhBXtPXgEAjO/ti7eHdoSVBYddExFR7bHMkCSOXC7C5PjDuHStHNaWcrzzTCCeDW4pdSwiIjJBLDPU6DYmZuLt74+gqkYHHydbfDK2Gzp6OUgdi4iITBTLDDWayhot5m85hvUJFwEAT7Z3w/vPd4HK1lLiZEREZMpYZqhRZBWWY0r8YaReKoJMBvzfgHZ45Yk2kMs57JqIiB4Oyww1uN9P5+PV9ckoKK2CysYSH4zugn4BblLHIiKiJoJlhhqMEAKf7D2Lf/98AjoBqL0c8MnYbvB2spU6GhERNSEsM9Qgiiuq8cY3qfj5aC4A4LluLfGvpzvB2lIhcTIiImpqWGao3p3MLcbkrw7jbH4prBRyzBvREX/v4cPbEhARUYNgmaF69d/ULLz1XRrKqrTwVFkjdmw3dPF2lDoWERE1YSwzVC+qtTos+ekEvth/DgDQx98ZK8Z0hbO9UuJkRETU1LHM0EPLK67AK+uSkXCuAAAw+XF/vDGwHSwUvC0BERE1PJYZeiiJ5wswdW0S8oorYa+0wLt/C8KgTp5SxyIiIjPCMkMPRAiBuD/O41/bjqNGJ9DWzR6fjOsGf1d7qaMREZGZYZmhOiurqsGcTen4PiULADA0yBNLRwXBTsndiYiIGh8/fahOzueXYnL8YZzIKYZCLsPswe3x4qN+HHZNRESSYZmhWvv1WC7+b2MKiitq4GJvhY/+HoxerZ2ljkVERGaOZYbuS6sTWP7rSazYdRoAEOzjiJVh3eChspY4GRERESDp2FmtVouoqCj4+fnBxsYG/v7+WLRoEYQQ+mU2bdqEgQMHwtnZGTKZDCkpKdIFNkPXSqswYXWCvshM6NMKG17uzSJDRERGQ9IjMzExMYiNjUVcXBzUajUSExMxceJEqFQqTJ8+HQBQWlqKRx99FM8//zxeeuklKeOanfRLRZgcfxiXC8thbSlH9LOBeKZrS6ljERERGZC0zPzxxx8YOXIkhg4dCgBo1aoV1q9fj4SEBP0y48aNAwCcP3++VuusrKxEZWWl/meNRlN/gc3IxkOZePuHI6iq0cHX2RafjO2GDp4OUsciIiK6jaRfM/Xp0wc7d+7EyZMnAQCpqanYv38/Bg8e/MDrjI6Ohkql0j+8vb3rK65ZqKzRYvamdLz5XRqqanTo394NW155lEWGiIiMlqRHZiIjI6HRaNC+fXsoFApotVosXrwYYWFhD7zO2bNnY+bMmfqfNRoNC00tXS4sx9T4w0i9VASZDJg5oB2mPdEGcjmHXRMRkfGStMxs3LgRa9euxbp166BWq5GSkoIZM2bAy8sL4eHhD7ROpVIJpZI3N6yr/afy8er6JFwrq4ajrSU+GN0Vj7dzlToWERHRfUlaZmbNmoXIyEiMHj0aABAYGIgLFy4gOjr6gcsM1Y0QArF7z+DdnzOgE0CnFg6IDesGbydbqaMRERHViqRlpqysDHK54Wk7CoUCOp1OokTmRVNRjTc2puKXY7kAgL91a4lFT3eCtaVC4mRERES1J2mZGT58OBYvXgwfHx+o1WokJydj2bJliIiI0C9TUFCAixcvIivr+n2AMjIyAAAeHh7w8PCQJHdTcDK3GJO/Ooyz+aWwUsixYKQao0O8eVsCIiIyOTJx8xXqGllxcTGioqKwefNm5OXlwcvLC2PGjMHcuXNhZWUFAFizZg0mTpx423PnzZuH+fPn3/c1NBoNVCoVioqK4ODAETkAsCU1C299m4byai28VNaIHdsNnb0dpY5FRESkV5fPb0nLTGNgmflLtVaH6B9PYNXv5wAAj7Rxxoeju8LZnidMExGRcanL5zfvzWQm8oor8MraZCScLwAATO3nj9cHBkDBYddERGTiWGbMQOL5Akxdm4S84krYKy3w7t86Y1Annm9ERERNA8tMEyaEwJo/zmPxtuOo0Qm0dbPHp+O6obWrvdTRiIiI6g3LTBNVVlWD2ZvS8UPK9VFgw4I8ETMqCHZK/icnIqKmhZ9sTdC5/FJM/uowMnKLoZDLMGdIB0Q80orDromIqElimWlidhzLxcyvU1BcWQMXeyVWhgWjh5+T1LGIiIgaDMtME6HVCby/4yQ+2n0aANDdtzk+DguGu4O1xMmIiIgaFstME3CttArTNyTjt1P5AIAJfVphzpAOsLKQ3+eZREREpo9lxsSlXSrElPgkXC4sh42lAktGBWJklxZSxyIiImo0LDMm7OtDFxH1w1FU1ejQytkWn4zrhvYe5n2VYyIiMj8sMyaoolqL+VuOYsOhTADAgA5ueO/5LlDZWEqcjIiIqPGxzJiYS9fKMHVtEtIuFUEmA15/qh2m9msDOW9LQEREZoplxoT8duoKpq9PxrWyajjaWuLD0V3Rt52r1LGIiIgkxTJjAnQ6gdi9Z/DeLxnQCSCwhQorw4Lh7WQrdTQiIiLJscwYOU1FNV7fmIodx3IBAC9098aCkWpYWyokTkZERGQcWGaMWEZOMSZ9lYjzV8tgpZBj4Ug1RvfwkToWERGRUWGZMVI/pFxG5HfpKK/WooWjDVaGBaOzt6PUsYiIiIwOy4yRqdbq8M6Px7H69/MAgEfbuODDMV3hZGclbTAiIiIjxTJjRPI0FZi2LgmHzl8DAEx7wh8znwqAgsOuiYiI7oplxkgknCvAtHVJuFJciWZKC7z3fGcMVHtIHYuIiMjoscxITAiB1b+fxzs/HkeNTqCduz0+GdsNrV3tpY5GRERkElhmJFRWVYPI79KxJTULADC8sxdiRgXC1or/WYiIiGqLn5oSOZdfislfHUZGbjEs5DLMGdIBEx9pBZmM58cQERHVBcuMBH45moPXN6aiuLIGrs2U+Pjvwejh5yR1LCIiIpPEMtOItDqB937JwMo9ZwAAIa2a4+O/B8PNwVriZERERKaLZaaRFJRWYfr6ZOw/nQ8AmPhIK8wZ0gGWCrnEyYiIiEwby0wjSM0sxNS1SbhcWA4bSwWWjArEyC4tpI5FRETUJLDMNLANCRcx94ejqNLq0MrZFp+M64b2Hg5SxyIiImoyWGYaSEW1FvN+OIqvEzMBAE91dMd7z3eGg7WlxMmIiIiaFpaZBnDpWhmmxCch/XIR5DLg9YEBmPK4P+S8LQEREVG9Y5mpZ/tOXsH0DckoLKtGc1tLfDimKx5r6yp1LCIioiaLZaae6HQCK/ecxns7TkIIIKilCivDgtGyua3U0YiIiJo0lpl6UFRejdc3puLX47kAgNEh3pg/Qg1rS4XEyYiIiJo+lpmHdCJHg8lfHcb5q2WwspBj4Qg1RvfwkToWERGR2WCZeQg/pFxG5HfpKK/WooWjDWLHBiOopaPUsYiIiMwKy8wD+vfPJ/Dx7uu3JXisrQs+GN0VTnZWEqciIiIyPywzD6iLd3PIZMC0fm3wf0+1g4LDromIiCTBMvOAnurojl9nPg5/V3upoxAREZk13uXwIbDIEBERSY9lhoiIiEwaywwRERGZNJYZIiIiMmksM0RERGTSWGaIiIjIpLHMEBERkUljmSEiIiKTxjJDREREJo1lhoiIiEwaywwRERGZNEnLjFarRVRUFPz8/GBjYwN/f38sWrQIQgj9MkIIzJ07F56enrCxscGAAQNw6tQpCVMTERGRMZG0zMTExCA2NhYfffQRjh8/jpiYGCxduhQrVqzQL7N06VJ8+OGH+OSTT3Dw4EHY2dkhNDQUFRUVEiYnIiIiYyETNx8GaWTDhg2Du7s7vvjiC/20UaNGwcbGBvHx8RBCwMvLC6+//jreeOMNAEBRURHc3d2xZs0ajB49+r6vodFooFKpUFRUBAcHhwZ7L0RERFR/6vL5bdFIme6oT58++Oyzz3Dy5Em0a9cOqamp2L9/P5YtWwYAOHfuHHJycjBgwAD9c1QqFXr27IkDBw7cscxUVlaisrJS/3NRURGA6xuFiIiITMONz+3aHHORtMxERkZCo9Ggffv2UCgU0Gq1WLx4McLCwgAAOTk5AAB3d3eD57m7u+vn3So6OhoLFiy4bbq3t3c9pyciIqKGVlxcDJVKdc9lJC0zGzduxNq1a7Fu3Tqo1WqkpKRgxowZ8PLyQnh4+AOtc/bs2Zg5c6b+Z51Oh4KCAjg7O0Mmk9VXdADXW6O3tzcyMzP5FdZ9cFvVHrdV7XFb1R63Ve1xW9VeQ24rIQSKi4vh5eV132UlLTOzZs1CZGSk/uuiwMBAXLhwAdHR0QgPD4eHhwcAIDc3F56envrn5ebmokuXLndcp1KphFKpNJjm6OjYIPlvcHBw4A5fS9xWtcdtVXvcVrXHbVV73Fa111Db6n5HZG6QdDRTWVkZ5HLDCAqFAjqdDgDg5+cHDw8P7Ny5Uz9fo9Hg4MGD6N27d6NmJSIiIuMk6ZGZ4cOHY/HixfDx8YFarUZycjKWLVuGiIgIAIBMJsOMGTPwr3/9C23btoWfnx+ioqLg5eWFp59+WsroREREZCQkLTMrVqxAVFQUpk6diry8PHh5eWHSpEmYO3eufpk333wTpaWlePnll1FYWIhHH30U27dvh7W1tYTJr1MqlZg3b95tX2vR7bitao/bqva4rWqP26r2uK1qz1i2laTXmSEiIiJ6WLw3ExEREZk0lhkiIiIyaSwzREREZNJYZoiIiMikmX2Z2bdvH4YPHw4vLy/IZDJ8//33BvOFEJg7dy48PT1hY2ODAQMG4NSpUwbLFBQUICwsDA4ODnB0dMSLL76IkpISg2XS0tLw2GOPwdraGt7e3li6dGlDv7V6d79tNWHCBMhkMoPHoEGDDJYxh20VHR2NkJAQNGvWDG5ubnj66aeRkZFhsExFRQWmTZsGZ2dn2NvbY9SoUcjNzTVY5uLFixg6dChsbW3h5uaGWbNmoaamxmCZPXv2IDg4GEqlEm3atMGaNWsa+u3Vu9psr379+t22b02ePNlgGXPYXrGxsQgKCtJfoKx379746aef9PO5X/3lftuK+9TdLVmyRH9plBuMft8SZu7HH38U//znP8WmTZsEALF582aD+UuWLBEqlUp8//33IjU1VYwYMUL4+fmJ8vJy/TKDBg0SnTt3Fn/++af47bffRJs2bcSYMWP084uKioS7u7sICwsTR44cEevXrxc2Njbi008/bay3WS/ut63Cw8PFoEGDRHZ2tv5RUFBgsIw5bKvQ0FCxevVqceTIEZGSkiKGDBkifHx8RElJiX6ZyZMnC29vb7Fz506RmJgoevXqJfr06aOfX1NTIzp16iQGDBggkpOTxY8//ihcXFzE7Nmz9cucPXtW2NraipkzZ4pjx46JFStWCIVCIbZv396o7/dh1WZ7Pf744+Kll14y2LeKior0881le23ZskVs27ZNnDx5UmRkZIg5c+YIS0tLceTIESEE96ub3W9bcZ+6s4SEBNGqVSsRFBQkXnvtNf10Y9+3zL7M3OzWD2idTic8PDzEv//9b/20wsJCoVQqxfr164UQQhw7dkwAEIcOHdIv89NPPwmZTCYuX74shBBi5cqVonnz5qKyslK/zFtvvSUCAgIa+B01nLuVmZEjR971Oea6rfLy8gQAsXfvXiHE9X3I0tJSfPPNN/pljh8/LgCIAwcOCCGuF0e5XC5ycnL0y8TGxgoHBwf9tnnzzTeFWq02eK0XXnhBhIaGNvRbalC3bi8hrn/w3PyH9VbmvL2aN28u/vOf/3C/qoUb20oI7lN3UlxcLNq2bSt27NhhsH1MYd8y+6+Z7uXcuXPIycnBgAED9NNUKhV69uyJAwcOAAAOHDgAR0dHdO/eXb/MgAEDIJfLcfDgQf0yffv2hZWVlX6Z0NBQZGRk4Nq1a430bhrHnj174ObmhoCAAEyZMgVXr17VzzPXbVVUVAQAcHJyAgAcPnwY1dXVBvtV+/bt4ePjY7BfBQYGGtwxPjQ0FBqNBkePHtUvc/M6bixzYx2m6tbtdcPatWvh4uKCTp06Yfbs2SgrK9PPM8ftpdVqsWHDBpSWlqJ3797cr+7h1m11A/cpQ9OmTcPQoUNve0+msG9JegVgY5eTkwMABv9xbvx8Y15OTg7c3NwM5ltYWMDJyclgGT8/v9vWcWNe8+bNGyR/Yxs0aBCeffZZ+Pn54cyZM5gzZw4GDx6MAwcOQKFQmOW20ul0mDFjBh555BF06tQJwPX3YWVlddsNUG/dr+60392Yd69lNBoNysvLYWNj0xBvqUHdaXsBwN///nf4+vrCy8sLaWlpeOutt5CRkYFNmzYBMK/tlZ6ejt69e6OiogL29vbYvHkzOnbsiJSUFO5Xt7jbtgK4T91qw4YNSEpKwqFDh26bZwp/s1hmqN7cuPs5cP0O6EFBQfD398eePXvQv39/CZNJZ9q0aThy5Aj2798vdRSTcLft9fLLL+v/HRgYCE9PT/Tv3x9nzpyBv79/Y8eUVEBAAFJSUlBUVIRvv/0W4eHh2Lt3r9SxjNLdtlXHjh25T90kMzMTr732Gnbs2GEUtwp6EPya6R48PDwA4LYztnNzc/XzPDw8kJeXZzC/pqYGBQUFBsvcaR03v0ZT1Lp1a7i4uOD06dMAzG9bvfLKK9i6dSt2796Nli1b6qd7eHigqqoKhYWFBsvful/dbzvcbRkHBweT+j/CG+62ve6kZ8+eAGCwb5nL9rKyskKbNm3QrVs3REdHo3Pnzvjggw+4X93B3bbVnZjzPnX48GHk5eUhODgYFhYWsLCwwN69e/Hhhx/CwsIC7u7uRr9vsczcg5+fHzw8PLBz5079NI1Gg4MHD+q/d+3duzcKCwtx+PBh/TK7du2CTqfT/3L07t0b+/btQ3V1tX6ZHTt2ICAgwOS+NqmLS5cu4erVq/D09ARgPttKCIFXXnkFmzdvxq5du2772qxbt26wtLQ02K8yMjJw8eJFg/0qPT3doPzt2LEDDg4O+sPkvXv3NljHjWVuPifAFNxve91JSkoKABjsW+ayvW6l0+lQWVnJ/aoWbmyrOzHnfap///5IT09HSkqK/tG9e3eEhYXp/230+9ZDn0Js4oqLi0VycrJITk4WAMSyZctEcnKyuHDhghDi+tBsR0dH8cMPP4i0tDQxcuTIOw7N7tq1qzh48KDYv3+/aNu2rcFw48LCQuHu7i7GjRsnjhw5IjZs2CBsbW1NarixEPfeVsXFxeKNN94QBw4cEOfOnRO//vqrCA4OFm3bthUVFRX6dZjDtpoyZYpQqVRiz549BsM+y8rK9MtMnjxZ+Pj4iF27donExETRu3dv0bt3b/38G8McBw4cKFJSUsT27duFq6vrHYc5zpo1Sxw/flx8/PHHJjks9H7b6/Tp02LhwoUiMTFRnDt3Tvzwww+idevWom/fvvp1mMv2ioyMFHv37hXnzp0TaWlpIjIyUshkMvHLL78IIbhf3exe24r71P3dOtrL2Pctsy8zu3fvFgBue4SHhwshrg/PjoqKEu7u7kKpVIr+/fuLjIwMg3VcvXpVjBkzRtjb2wsHBwcxceJEUVxcbLBMamqqePTRR4VSqRQtWrQQS5Ysaay3WG/uta3KysrEwIEDhaurq7C0tBS+vr7ipZdeMhimJ4R5bKs7bSMAYvXq1fplysvLxdSpU0Xz5s2Fra2teOaZZ0R2drbBes6fPy8GDx4sbGxshIuLi3j99ddFdXW1wTK7d+8WXbp0EVZWVqJ169YGr2Eq7re9Ll68KPr27SucnJyEUqkUbdq0EbNmzTK4JogQ5rG9IiIihK+vr7CyshKurq6if//++iIjBPerm91rW3Gfur9by4yx71syIYR4+OM7RERERNLgOTNERERk0lhmiIiIyKSxzBAREZFJY5khIiIik8YyQ0RERCaNZYaIiIhMGssMERERmTSWGSIiIjJpLDNE9NDOnz8PmUymv7+NMThx4gR69eoFa2trdOnSReo49ySTyfD9999LHYPIZLHMEDUBEyZMgEwmw5IlSwymf//995DJZBKlkta8efNgZ2eHjIyM225ud8ON7XbrY9CgQY2clogeBssMURNhbW2NmJgYXLt2Teoo9aaqquqBn3vmzBk8+uij8PX1hbOz812XGzRoELKzsw0e69evf+DXJaLGxzJD1EQMGDAAHh4eiI6Ovusy8+fPv+0rl+XLl6NVq1b6nydMmICnn34a77zzDtzd3eHo6IiFCxeipqYGs2bNgpOTE1q2bInVq1fftv4TJ06gT58+sLa2RqdOnbB3716D+UeOHMHgwYNhb28Pd3d3jBs3Dvn5+fr5/fr1wyuvvIIZM2bAxcUFoaGhd3wfOp0OCxcuRMuWLaFUKtGlSxds375dP18mk+Hw4cNYuHAhZDIZ5s+ff9dtolQq4eHhYfBo3ry5wbpiY2MxePBg2NjYoHXr1vj2228N1pGeno4nn3wSNjY2cHZ2xssvv4ySkhKDZVatWgW1Wg2lUglPT0+88sorBvPz8/PxzDPPwNbWFm3btsWWLVv0865du4awsDC4urrCxsYGbdu2veP2JzJXLDNETYRCocA777yDFStW4NKlSw+1rl27diErKwv79u3DsmXLMG/ePAwbNgzNmzfHwYMHMXnyZEyaNOm215k1axZef/11JCcno3fv3hg+fDiuXr0KACgsLMSTTz6Jrl27IjExEdu3b0dubi6ef/55g3XExcXBysoKv//+Oz755JM75vvggw/w3nvv4d1330VaWhpCQ0MxYsQInDp1CgCQnZ0NtVqN119/HdnZ2XjjjTceantERUVh1KhRSE1NRVhYGEaPHo3jx48DAEpLSxEaGormzZvj0KFD+Oabb/Drr78alJXY2FhMmzYNL7/8MtLT07Flyxa0adPG4DUWLFiA559/HmlpaRgyZAjCwsJQUFCgf/1jx47hp59+wvHjxxEbGwsXF5eHek9ETUq93HubiCQVHh4uRo4cKYQQolevXiIiIkIIIcTmzZvFzb/m8+bNE507dzZ47vvvvy98fX0N1uXr6yu0Wq1+WkBAgHjsscf0P9fU1Ag7Ozuxfv16IYQQ586dEwDEkiVL9MtUV1eLli1bipiYGCGEEIsWLRIDBw40eO3MzEwBQGRkZAghhHj88cdF165d7/t+vby8xOLFiw2mhYSEiKlTp+p/7ty5s5g3b9491xMeHi4UCoWws7MzeNy8bgBi8uTJBs/r2bOnmDJlihBCiM8++0w0b95clJSU6Odv27ZNyOVykZOTo8/7z3/+8645AIi3335b/3NJSYkAIH766SchhBDDhw8XEydOvOd7ITJnFlIWKSKqfzExMXjyyScf6miEWq2GXP7XgVt3d3d06tRJ/7NCoYCzszPy8vIMnte7d2/9vy0sLNC9e3f9EYzU1FTs3r0b9vb2t73emTNn0K5dOwBAt27d7plNo9EgKysLjzzyiMH0Rx55BKmpqbV8h3954oknEBsbazDNycnJ4Oeb39eNn2+M3Dp+/Dg6d+4MOzs7gyw6nQ4ZGRmQyWTIyspC//7975kjKChI/287Ozs4ODjot++UKVMwatQoJCUlYeDAgXj66afRp0+fOr9XoqaKZYaoienbty9CQ0Mxe/ZsTJgwwWCeXC6HEMJgWnV19W3rsLS0NPhZJpPdcZpOp6t1rpKSEgwfPhwxMTG3zfP09NT/++ZS0Bjs7Oxu+8qnPtnY2NRquXtt38GDB+PChQv48ccfsWPHDvTv3x/Tpk3Du+++W+95iUwRz5khaoKWLFmC//73vzhw4IDBdFdXV+Tk5BgUmvq8Nsyff/6p/3dNTQ0OHz6MDh06AACCg4Nx9OhRtGrVCm3atDF41KXAODg4wMvLC7///rvB9N9//x0dO3asnzdyi5vf142fb7yvDh06IDU1FaWlpQZZ5HI5AgIC0KxZM7Rq1equw8Nry9XVFeHh4YiPj8fy5cvx2WefPdT6iJoSlhmiJigwMBBhYWH48MMPDab369cPV65cwdKlS3HmzBl8/PHH+Omnn+rtdT/++GNs3rwZJ06cwLRp03Dt2jVEREQAAKZNm4aCggKMGTMGhw4dwpkzZ/Dzzz9j4sSJ0Gq1dXqdWbNmISYmBl9//TUyMjIQGRmJlJQUvPbaa3XOXFlZiZycHIPHzSOsAOCbb77BqlWrcPLkScybNw8JCQn6E3zDwsJgbW2N8PBwHDlyBLt378arr76KcePGwd3dHcD1UWTvvfcePvzwQ5w6dQpJSUlYsWJFrTPOnTsXP/zwA06fPo2jR49i69at+jJFRCwzRE3WwoULb/saqEOHDli5ciU+/vhjdO7cGQkJCQ890udmS5YswZIlS9C5c2fs378fW7Zs0Y+6uXE0RavVYuDAgQgMDMSMGTPg6OhocH5ObUyfPh0zZ87E66+/jsDAQGzfvh1btmxB27Zt65x5+/bt8PT0NHg8+uijBsssWLAAGzZsQFBQEL788kusX79efxTI1tYWP//8MwoKChASEoLnnnsO/fv3x0cffaR/fnh4OJYvX46VK1dCrVZj2LBh+pFXtWFlZYXZs2cjKCgIffv2hUKhwIYNG+r8XomaKpm49Qt0IiLSk8lk2Lx5M55++mmpoxDRXfDIDBEREZk0lhkiIiIyaRyaTUR0D/wmnsj48cgMERERmTSWGSIiIjJpLDNERERk0lhmiIiIyKSxzBAREZFJY5khIiIik8YyQ0RERCaNZYaIiIhM2v8DGiG+I5cTBcUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "\n",
    "learning_rate = 0.01\n",
    "lower_drop_rate = 5\n",
    "upper_drop_rate = 10 #This means no dropout\n",
    "\n",
    "mynn = MyNeuralNetwork()\n",
    "mynn.add_input(InputLayer(\"In\",64))\n",
    "mynn.add_hidden(HiddenLayer(\"H1\",5,SigmoidActivation(),GradientDescentOptimizer(),learning_rate))\n",
    "mynn.add_output(OutputLayer(\"Out1\",10,SoftmaxActivation(),GradientDescentOptimizer(),learning_rate))\n",
    "\n",
    "\n",
    "master_accuracy_log = np.empty((0,2),float)\n",
    "\n",
    "for i in range(1000,5000,1000):\n",
    "    mynn.build()\n",
    "    epochs = i\n",
    "    dropout_rates = np.random.randint(lower_drop_rate, upper_drop_rate, epochs)\n",
    "    loss_log,accuracy_log = mynn.train(X_train,y_train,dropout_rates,epochs)\n",
    "    accuracy = mynn.predict(X_test,y_test) * 100\n",
    "    master_accuracy_log = np.append(master_accuracy_log,np.array([[epochs,accuracy]]),0)\n",
    "\n",
    "# The next step is to show the loss\n",
    "plt.figure()\n",
    "#plt.plot(np.arange(len(loss_log)), loss_log,label=\"Loss\")\n",
    "plt.plot(master_accuracy_log[:,0], master_accuracy_log[:,1])\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0c36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
